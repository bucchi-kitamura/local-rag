#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
=====================
PDFèª­ã¿è¾¼ã¿ã€ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã€æ¤œç´¢ã€LLMã«ã‚ˆã‚‹å›ç­”ç”Ÿæˆã¾ã§ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æä¾›
"""

import os
import json
from typing import List, Dict, Any, Optional

from langchain_ollama import OllamaLLM
from langchain.prompts import PromptTemplate

from document_loader import DocumentProcessor
from embedder import Embedder

class RAGPipeline:
    """PDFã‹ã‚‰RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(
        self,
        llm_model: str = "gemma:7b",
        embed_model: str = "intfloat/multilingual-e5-small",
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        models_dir: str = "models",
    ):
        """
        RAGPipelineã®ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿
        
        Args:
            llm_model: ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«å
            embed_model: ä½¿ç”¨ã™ã‚‹åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«å
            chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆæ–‡å­—æ•°ï¼‰
            chunk_overlap: ãƒãƒ£ãƒ³ã‚¯é–“ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ï¼ˆæ–‡å­—æ•°ï¼‰
            models_dir: ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        """
        self.llm_model = llm_model
        self.embed_model = embed_model
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.models_dir = models_dir
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        self.document_processor = DocumentProcessor(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        self.embedder = Embedder(model_name=embed_model)
        self.llm = OllamaLLM(model=llm_model)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®è¨­å®š
        self.prompt_template = PromptTemplate(
            input_variables=["context", "question"],
            template="""
            è³ªå•ã«å¯¾ã—ã¦ã€ä»¥ä¸‹ã®æƒ…å ±ã«åŸºã¥ã„ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚

            æƒ…å ±:
            {context}

            è³ªå•: {question}

            å›ç­”:
            """
        )
        
        # LangChain v0.2ã®æ–°ã—ã„è¨˜æ³•ã§ãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆ
        self.llm_chain = self.prompt_template | self.llm
        
        # çŠ¶æ…‹ç®¡ç†ç”¨ã®å¤‰æ•°
        self.is_index_built = False
        self.documents = []
        
    def build_index_from_pdf(
        self,
        pdf_path: str,
        save_index: bool = True,
        index_name: Optional[str] = None,
    ) -> None:
        """
        PDFã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ã™ã‚‹
        
        Args:
            pdf_path: PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            save_index: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹
            index_name: ä¿å­˜ã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®åå‰
        """
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‚’æŠ½å‡ºã—ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¨ã—ã¦ä½¿ç”¨
        if index_name is None:
            basename = os.path.basename(pdf_path)
            index_name = os.path.splitext(basename)[0]
        
        # PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¦ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        self.documents = self.document_processor.load_and_split(pdf_path)
        
        # ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
        self.embedder.create_index(self.documents)
        self.is_index_built = True
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜
        if save_index:
            self.embedder.save_index(self.models_dir, name=index_name)
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚ä¿å­˜
            metadata_path = os.path.join(self.models_dir, f"{index_name}_documents.json")
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(self.documents, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {metadata_path}")
    
    def load_index(self, index_name: str) -> None:
        """
        ä¿å­˜ã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã‚€
        
        Args:
            index_name: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®åå‰
        """
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã‚€
        self.embedder.load_index(self.models_dir, name=index_name)
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚èª­ã¿è¾¼ã‚€
        metadata_path = os.path.join(self.models_dir, f"{index_name}_documents.json")
        if os.path.exists(metadata_path):
            with open(metadata_path, 'r', encoding='utf-8') as f:
                self.documents = json.load(f)
            self.embedder.documents = self.documents
            print(f"ğŸ“‚ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {metadata_path}")
            self.is_index_built = True
        else:
            raise FileNotFoundError(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {metadata_path}")
    
    def answer_question(self, question: str, top_k: int = 3) -> Dict[str, Any]:
        """
        è³ªå•ã«å¯¾ã—ã¦å›ç­”ã‚’ç”Ÿæˆã™ã‚‹
        
        Args:
            question: è³ªå•ãƒ†ã‚­ã‚¹ãƒˆ
            top_k: ä½¿ç”¨ã™ã‚‹é¡ä¼¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            
        Returns:
            å›ç­”ã¨ãã®ç”Ÿæˆã«ä½¿ç”¨ã•ã‚ŒãŸæƒ…å ±ã‚’å«ã‚€ãƒ‡ã‚£ã‚¯ã‚·ãƒ§ãƒŠãƒª
        """
        if not self.is_index_built:
            raise ValueError("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å…ˆã«build_index_from_pdfã¾ãŸã¯load_indexã‚’å‘¼ã³å‡ºã—ã¦ãã ã•ã„ã€‚")
        
        # è³ªå•ã«é–¢é€£ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢
        search_results = self.embedder.search(question, top_k=top_k)
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’çµåˆ
        contexts = []
        for i, result in enumerate(search_results):
            doc = result["document"]
            score = result["score"]
            content = doc["content"]
            source = doc["metadata"]["source"]
            page = doc["metadata"]["page"]
            
            context = f"[ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ {i+1}] (ã‚½ãƒ¼ã‚¹: {source}, ãƒšãƒ¼ã‚¸: {page})\n{content}\n"
            contexts.append(context)
        
        context_text = "\n".join(contexts)
        
        # LLMã«å›ç­”ã‚’ç”Ÿæˆã•ã›ã‚‹
        answer = self.llm_chain.invoke({
            "context": context_text,
            "question": question
        })
        
        return {
            "answer": answer,
            "sources": search_results,
            "context": context_text,
        }