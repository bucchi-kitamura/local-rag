#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
LlamaIndex RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
===============================
LlamaIndexã‚’ä½¿ç”¨ã—ãŸPDFèª­ã¿è¾¼ã¿ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã€è³ªå•å¿œç­”ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
"""

import os
from typing import List, Dict, Any, Optional
from pathlib import Path

from llama_index.core import (
    Document, 
    VectorStoreIndex, 
    StorageContext,
    load_index_from_storage,
    Settings
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.prompts import PromptTemplate
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

from llamaindex_document_loader import LlamaIndexDocumentProcessor


class LlamaIndexRAGPipeline:
    """LlamaIndexã‚’ä½¿ç”¨ã—ãŸRAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    def __init__(
        self,
        llm_model: str = "gemma:7b",
        embed_model: str = "intfloat/multilingual-e5-small",
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        models_dir: str = "models",
    ):
        """
        LlamaIndexRAGPipelineã®ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿
        
        Args:
            llm_model: ä½¿ç”¨ã™ã‚‹Ollamaã®LLMãƒ¢ãƒ‡ãƒ«å
            embed_model: ä½¿ç”¨ã™ã‚‹åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«å
            chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆæ–‡å­—æ•°ï¼‰
            chunk_overlap: ãƒãƒ£ãƒ³ã‚¯é–“ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ï¼ˆæ–‡å­—æ•°ï¼‰
            models_dir: ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        """
        self.llm_model = llm_model
        self.embed_model = embed_model
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.models_dir = models_dir

        # LLMã®è¨­å®šï¼ˆæ—¥æœ¬èªãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å¼·åˆ¶ï¼‰
        self.llm = Ollama(
            model=llm_model, 
            request_timeout=300.0,
            system_prompt="ã‚ãªãŸã¯æ—¥æœ¬èªã§å›ç­”ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚å¿…ãšæ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚è‹±èªã§ã®å›ç­”ã¯ç¦æ­¢ã§ã™ã€‚"
        )
        
        # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®è¨­å®š
        self.embedding = HuggingFaceEmbedding(model_name=embed_model)
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®šã‚’è¡Œã†
        Settings.llm = self.llm
        Settings.embed_model = self.embedding
        Settings.node_parser = SentenceSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
        )

        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        self.document_processor = LlamaIndexDocumentProcessor()
        self.index = None
        self.query_engine = None
        
        # çŠ¶æ…‹ç®¡ç†
        self.is_index_built = False

    def build_index_from_pdf(
        self,
        pdf_path: str,
        save_index: bool = True,
        index_name: Optional[str] = None,
    ) -> None:
        """
        PDFã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ã™ã‚‹
        
        Args:
            pdf_path: PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            save_index: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹
            index_name: ä¿å­˜ã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®åå‰
        """
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
        if index_name is None:
            basename = os.path.basename(pdf_path)
            index_name = os.path.splitext(basename)[0]

        # PDFã‹ã‚‰Documentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        documents = self.document_processor.load_documents(pdf_path)
        
        print(f"ğŸ§  LlamaIndexã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã¦ã„ã¾ã™...")
        
        # VectorStoreIndexã‚’ä½œæˆ
        self.index = VectorStoreIndex.from_documents(documents)
        
        # æ—¥æœ¬èªå›ç­”ç”¨ã®ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆã‚ˆã‚Šå¼·åŠ›ï¼‰
        qa_prompt_tmpl = (
            "ã‚ãªãŸã¯æ—¥æœ¬èªã§å›ç­”ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\n"
            "ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’å‚ç…§ã—ã¦ã€è³ªå•ã«æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚\n"
            "\n"
            "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±:\n"
            "---------------------\n"
            "{context_str}\n"
            "---------------------\n"
            "\n"
            "é‡è¦ãªæŒ‡ç¤º:\n"
            "1. å¿…ãšæ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„\n"
            "2. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã«åŸºã¥ã„ã¦å›ç­”ã—ã¦ãã ã•ã„\n"
            "3. æƒ…å ±ãŒãªã„å ´åˆã¯ã€Œæä¾›ã•ã‚ŒãŸæƒ…å ±ã‹ã‚‰ã¯å›ç­”ã§ãã¾ã›ã‚“ã€ã¨æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„\n"
            "4. è‹±èªã§å›ç­”ã™ã‚‹ã“ã¨ã¯çµ¶å¯¾ã«é¿ã‘ã¦ãã ã•ã„\n"
            "\n"
            "è³ªå•: {query_str}\n"
            "å›ç­”ï¼ˆæ—¥æœ¬èªï¼‰: "
        )
        qa_prompt = PromptTemplate(qa_prompt_tmpl)
        
        # QueryEngineã‚’ä½œæˆï¼ˆæ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½¿ç”¨ï¼‰
        self.query_engine = self.index.as_query_engine(
            similarity_top_k=3,  # top_kè¨­å®š
            response_mode="compact",
            text_qa_template=qa_prompt
        )
        
        self.is_index_built = True
        print(f"  âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸ")

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜
        if save_index:
            storage_dir = os.path.join(self.models_dir, f"{index_name}_llamaindex")
            os.makedirs(storage_dir, exist_ok=True)
            
            self.index.storage_context.persist(persist_dir=storage_dir)
            print(f"ğŸ’¾ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {storage_dir}")

    def load_index(self, index_name: str) -> None:
        """
        ä¿å­˜ã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã‚€
        
        Args:
            index_name: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®åå‰
        """
        storage_dir = os.path.join(self.models_dir, f"{index_name}_llamaindex")
        
        if not os.path.exists(storage_dir):
            raise FileNotFoundError(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {storage_dir}")

        print(f"ğŸ“‚ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™: {storage_dir}")
        
        # StorageContextã‹ã‚‰èª­ã¿è¾¼ã¿
        storage_context = StorageContext.from_defaults(persist_dir=storage_dir)
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿
        self.index = load_index_from_storage(storage_context)
        
        # æ—¥æœ¬èªå›ç­”ç”¨ã®ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆã‚ˆã‚Šå¼·åŠ›ï¼‰
        qa_prompt_tmpl = (
            "ã‚ãªãŸã¯æ—¥æœ¬èªã§å›ç­”ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\n"
            "ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’å‚ç…§ã—ã¦ã€è³ªå•ã«æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚\n"
            "\n"
            "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±:\n"
            "---------------------\n"
            "{context_str}\n"
            "---------------------\n"
            "\n"
            "é‡è¦ãªæŒ‡ç¤º:\n"
            "1. å¿…ãšæ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„\n"
            "2. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã«åŸºã¥ã„ã¦å›ç­”ã—ã¦ãã ã•ã„\n"
            "3. æƒ…å ±ãŒãªã„å ´åˆã¯ã€Œæä¾›ã•ã‚ŒãŸæƒ…å ±ã‹ã‚‰ã¯å›ç­”ã§ãã¾ã›ã‚“ã€ã¨æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„\n"
            "4. è‹±èªã§å›ç­”ã™ã‚‹ã“ã¨ã¯çµ¶å¯¾ã«é¿ã‘ã¦ãã ã•ã„\n"
            "\n"
            "è³ªå•: {query_str}\n"
            "å›ç­”ï¼ˆæ—¥æœ¬èªï¼‰: "
        )
        qa_prompt = PromptTemplate(qa_prompt_tmpl)
        
        # QueryEngineã‚’ä½œæˆï¼ˆæ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½¿ç”¨ï¼‰
        self.query_engine = self.index.as_query_engine(
            similarity_top_k=3,
            response_mode="compact",
            text_qa_template=qa_prompt
        )
        
        self.is_index_built = True
        print(f"  âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸ")

    def answer_question(self, question: str) -> Dict[str, Any]:
        """
        è³ªå•ã«å¯¾ã—ã¦å›ç­”ã‚’ç”Ÿæˆã™ã‚‹
        
        Args:
            question: è³ªå•ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            å›ç­”ã¨ãã®ç”Ÿæˆã«ä½¿ç”¨ã•ã‚ŒãŸæƒ…å ±ã‚’å«ã‚€ãƒ‡ã‚£ã‚¯ã‚·ãƒ§ãƒŠãƒª
        """
        if not self.is_index_built or self.query_engine is None:
            raise ValueError("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å…ˆã«build_index_from_pdfã¾ãŸã¯load_indexã‚’å‘¼ã³å‡ºã—ã¦ãã ã•ã„ã€‚")
        
        # QueryEngineã§è³ªå•ã«å›ç­”
        response = self.query_engine.query(question)
        
        # ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’æŠ½å‡º
        sources = []
        if hasattr(response, 'source_nodes') and response.source_nodes:
            for i, node in enumerate(response.source_nodes):
                source_info = {
                    "document": {
                        "id": node.node.node_id,
                        "content": node.node.text,
                        "metadata": node.node.metadata if node.node.metadata else {}
                    },
                    "score": getattr(node, 'score', 0.0)
                }
                sources.append(source_info)
        
        return {
            "answer": str(response),
            "sources": sources,
            "context": self._format_context(sources),
        }

    def _format_context(self, sources: List[Dict[str, Any]]) -> str:
        """
        ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—ã«æ•´å½¢ã™ã‚‹
        
        Args:
            sources: ã‚½ãƒ¼ã‚¹æƒ…å ±ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            æ•´å½¢ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—
        """
        contexts = []
        for i, source in enumerate(sources):
            doc = source["document"]
            score = source["score"]
            content = doc["content"]
            metadata = doc["metadata"]
            
            source_name = metadata.get("source", "unknown")
            page = metadata.get("page", 0)
            
            context = f"[ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ {i+1}] (ã‚½ãƒ¼ã‚¹: {source_name}, ãƒšãƒ¼ã‚¸: {page}, ã‚¹ã‚³ã‚¢: {score:.4f})\\n{content}\\n"
            contexts.append(context)
        
        return "\\n".join(contexts)
